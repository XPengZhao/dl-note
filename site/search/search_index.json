{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"gpu/","title":"GPU Configuration","text":""},{"location":"gpu/#specific-gpus-explicitly","title":"Specific GPUs Explicitly","text":"<pre><code>torch.cuda.set_device(0,1)\n\n## or you can set the env variable\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n</code></pre> <p>or set the env variable when running the code:</p> <pre><code>CUDA_VISIBLE_DEVICES=0,1 python script.py\n</code></pre> <p>Comparison:</p> <p>If you want to restrict which GPUs are available to your script before it even starts, <code>CUDA_VISIBLE_DEVICES</code> is the way to go.</p> <p>If you want to change the active GPU dynamically based on some logic in your code, you'll need to use <code>torch.cuda.set_device()</code>.</p>"},{"location":"llm-basics/","title":"LLM Basics","text":""},{"location":"llm-basics/#kv-cache","title":"KV Cache","text":""},{"location":"llm-basics/#memory-cost","title":"Memory cost","text":"<p>The KV cache size is calculated as:</p> \\[ \\text{KV size} = 2 \\times \\text{num layers} \\times \\text{num heads} \\times \\text{seq len} \\times \\text{head dim} \\] <p>Assume:</p> <ul> <li>32 layers, 80 heads, head_dim = 128, seq_len = 4K.</li> <li>FP16 KV cache: 2 \u00d7 32 \u00d7 80 \u00d7 4K \u00d7 128 \u00d7 2 bytes = ~5.2 GB per sample.</li> <li>C8 (int8 KV cache): half that \u2192 ~2.6 GB per sample.</li> </ul>"},{"location":"llm-basics/#attention","title":"Attention","text":"<p>In transformer models, the attention module computes:</p> \\[ \\operatorname{Attention}(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V \\] <p>Every token produces:</p> <ul> <li>Q (Query) \u2014 derived from the current token\u2019s hidden state</li> <li>K (Key), V (Value) \u2014 derived from the context (previous tokens) and stored in the KV cache</li> </ul> <p>The difference between MHA, MQA, and GQA lies in how many Key/Value projections are used relative to the number of Query heads.</p>"},{"location":"llm-basics/#mha-multi-head-attention","title":"MHA \u2014 Multi-Head Attention","text":"<p>Each attention head has its own set of projections:</p> \\[ Q_i=X W_{Q, i}, \\quad K_i=X W_{K, i}, \\quad V_i=X W_{V, i} \\] <ul> <li>Q heads = K heads = V heads</li> </ul>"},{"location":"llm-basics/#mqa-multi-query-attention","title":"MQA \u2014 Multi-Query Attention","text":"<p>All heads share one K and V projection:</p> \\[ K_{\\text {shared }}=X W_K, \\quad V_{\\text {shared }}=X W_V \\] <ul> <li>Multiple Q heads, but only one pair of shared K/V.</li> <li>During inference, the same K/V are reused for all heads.</li> <li>KV cache size reduced by ~#heads\u00d7</li> <li>LLaMA 2 7B, Mistral, and most optimized LLMs use MQA to cut memory usage.</li> </ul>"},{"location":"llm-basics/#gqa-grouped-query-attention","title":"GQA \u2014 Grouped-Query Attention","text":"<p>Queries are split into groups, and each group shares its own K/V:</p> \\[ \\text { Group } g: \\quad Q_{g, *}, K_g, V_g \\] <ul> <li>Suppose you have 8 query heads but only 2 groups \u2192 4 Q heads per group share one K/V.</li> <li>K/V heads &lt; Q heads.</li> <li>LLaMA 2 70B, Mixtral, GPT-J-6B, etc. use GQA for better memory efficiency.</li> </ul>"},{"location":"neural-graphics/","title":"Neural Graphics","text":""},{"location":"neural-graphics/#neural-radiance-fields-nerf","title":"Neural Radiance Fields (NeRF)","text":"<p>Neural Radiance Fields (NeRF) represents a scene as a continuous volumetric field, where the density \\(\\sigma \\in \\mathbb{R}\\) and radiance \\(\\mathbf{c} \\in \\mathbb{R}^3\\) at any 3D position \\(\\mathbf{x} \\in \\mathbb{R}^3\\) under viewing direction \\(\\mathbf{d} \\in \\mathbb{R}^3\\) are modeled by a multi-layer perceptron (MLP) \\(f_\\theta : (\\mathbf{x}, \\mathbf{d}) \\rightarrow (c, \\sigma)\\), with \\(\\theta\\) as learnable parameters. To render a pixel, the MLP first evaluates points sampled from the camera ray \\(\\mathbf{r} = \\mathbf{o} + t\\mathbf{d}\\) to get their densities and radiance, and then the color \\(\\mathbf{C}(\\mathbf{r})\\) is estimated by volume rendering equation approximated using quadrature:</p> \\[ \\widehat{\\mathbf{C}}(\\mathbf{r} ; \\sigma, \\mathbf{c})=\\sum_k T_i(\\sigma)\\left(1-\\exp \\left(-\\sigma_i \\delta_i\\right)\\right) \\mathbf{c}_i \\] <p>where \\(\\delta_i = t_{i+1} \u2212 t_i\\) and \\(T_i(\\sigma)=\\exp \\left(-\\sum_{j&lt;i} \\sigma_j \\delta_j\\right)\\). \\(\\widehat{\\mathbf{C}}\\) conditioned on \\(\\sigma\\), \\(\\mathbf{c}\\), and \\(T\\) is conditioned on \\(\\sigma\\) to simplify follow-up descriptions. We denote the contribution of a point to the cumulative color as its weight \\(\\omega_i\\):</p> \\[ \\omega_i=T_i(\\sigma)\\left(1-\\exp \\left(-\\sigma_i \\delta_i\\right)\\right) \\] <p>NeRF is optimized by minimizing the photometric loss:</p> \\[ \\mathcal{L}_{p m}=\\|\\widehat{\\mathbf{C}}-\\mathbf{C}\\|_2 \\]"},{"location":"neural-graphics/#physical-meanings-of-the-terms-in-the-volume-rendering-equation","title":"Physical meanings of the terms in the volume rendering equation","text":"<p>Alpha value of a point: \\(\\alpha_i = 1-\\exp \\left(-\\sigma_i \\delta_i\\right)\\)</p> <p>Contribution of a point to the cumulative color: \\(\\omega_i = T_i \\alpha_i\\)</p> <p>Opacity of each ray: \\(\\tau = \\sum_i \\omega_i\\)</p> <p>Depth of each ray: \\(d = \\sum_i \\omega_i t_i\\), where \\(t_i\\) is the distance between the i-th point and the camera</p>"},{"location":"tensor/","title":"Tensor Operations","text":""},{"location":"tensor/#tensor-creation","title":"Tensor Creation","text":""},{"location":"tensor/#dimension-operations","title":"Dimension Operations","text":""},{"location":"tensor/#add-a-new-dimension","title":"Add a new dimension","text":"<p>unsqueeze()</p> <pre><code>t = torch.rand((3,4))  ## shaped [3, 4]\n\nt_new = t.unsqueeze(0) ## shaped [1, 3, 4]\nt_new = t.unsqueeze(1) ## shaped [3, 1, 4]\nt_new = t.unsqueeze(2) ## shaped [3, 4, 1]\n</code></pre> <p>view()</p> <pre><code>t = torch.rand(2)  ## shaped [2]\n\nt_new = t.view(1, 1, 2) ## shaped [1, 1, 2]\n</code></pre>"},{"location":"tensor/#activate-function","title":"Activate Function","text":""},{"location":"tensor/#sigmoid","title":"Sigmoid","text":"<p>torch.sigmoid</p> <p>Computes the logistic sigmoid function of the elements of input.</p> \\[ \\text{out}_i = \\frac{1}{1 + e^{-\\text{input}_i}} \\] <pre><code>t = torch.randn(4)\ntorch.sigmoid(t)l\n</code></pre>"}]}