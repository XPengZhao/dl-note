{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"hardware/","title":"Hardware","text":""},{"location":"hardware/#gpu-npu-gpgpu","title":"GPU, NPU, GPGPU","text":"<ul> <li> <p>GPU (Graphics Processing Unit): A massively parallel, general-purpose processor (SIMT/SIMD) built for high throughput across many workloads\u2014graphics, linear algebra, and most ML. Very flexible and programmable (CUDA/ROCm).</p> </li> <li> <p>NPU (Neural Processing Unit): A domain-specific accelerator optimized for neural-net kernels (matmul/conv, activation, pooling). Prioritizes energy efficiency and latency with fixed-function or semi-programmable dataflows.</p> </li> <li> <p>GPGPU (General-Purpose computing on Graphics Processing Units): It means using a GPU\u2019s massively parallel hardware to run non-graphics workloads (science/ML/signal processing, etc.). Instead of drawing triangles, you launch compute kernels that apply the same operation to many data elements at once (SIMT/SIMD). This gives huge throughput for data-parallel problems.</p> </li> </ul>"},{"location":"hardware/#hbm-gddr","title":"HBM, GDDR","text":"<p>GDDR (Graphics Double Data Rate) and HBM (High Bandwidth Memory) are two distinct types of memory technologies used in GPUs and high-performance computing systems, differing mainly in architecture, bandwidth, and power efficiency. GDDR uses a traditional planar design with memory chips arranged around the GPU, communicating over a relatively narrow bus at very high clock speeds to achieve high bandwidth. In contrast, HBM stacks multiple DRAM dies vertically and connects them to the processor using through-silicon vias (TSVs) and an interposer, creating an extremely wide memory interface that enables much higher bandwidth at lower clock speeds and reduced power consumption. As a result, HBM provides superior performance per watt and is often used in data centers and AI accelerators, while GDDR remains the preferred choice for consumer GPUs due to its lower cost and simpler integration.</p>"},{"location":"hardware/#gpu-configuration","title":"GPU Configuration","text":""},{"location":"hardware/#specific-gpus-explicitly","title":"Specific GPUs Explicitly","text":"<pre><code>torch.cuda.set_device(0,1)\n\n## or you can set the env variable\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n</code></pre> <p>or set the env variable when running the code:</p> <pre><code>CUDA_VISIBLE_DEVICES=0,1 python script.py\n</code></pre> <p>Comparison:</p> <p>If you want to restrict which GPUs are available to your script before it even starts, <code>CUDA_VISIBLE_DEVICES</code> is the way to go.</p> <p>If you want to change the active GPU dynamically based on some logic in your code, you'll need to use <code>torch.cuda.set_device()</code>.</p>"},{"location":"neural-graphics/","title":"Neural Graphics","text":""},{"location":"neural-graphics/#neural-radiance-fields-nerf","title":"Neural Radiance Fields (NeRF)","text":"<p>Neural Radiance Fields (NeRF) represents a scene as a continuous volumetric field, where the density \\(\\sigma \\in \\mathbb{R}\\) and radiance \\(\\mathbf{c} \\in \\mathbb{R}^3\\) at any 3D position \\(\\mathbf{x} \\in \\mathbb{R}^3\\) under viewing direction \\(\\mathbf{d} \\in \\mathbb{R}^3\\) are modeled by a multi-layer perceptron (MLP) \\(f_\\theta : (\\mathbf{x}, \\mathbf{d}) \\rightarrow (c, \\sigma)\\), with \\(\\theta\\) as learnable parameters. To render a pixel, the MLP first evaluates points sampled from the camera ray \\(\\mathbf{r} = \\mathbf{o} + t\\mathbf{d}\\) to get their densities and radiance, and then the color \\(\\mathbf{C}(\\mathbf{r})\\) is estimated by volume rendering equation approximated using quadrature:</p> \\[ \\widehat{\\mathbf{C}}(\\mathbf{r} ; \\sigma, \\mathbf{c})=\\sum_k T_i(\\sigma)\\left(1-\\exp \\left(-\\sigma_i \\delta_i\\right)\\right) \\mathbf{c}_i \\] <p>where \\(\\delta_i = t_{i+1} \u2212 t_i\\) and \\(T_i(\\sigma)=\\exp \\left(-\\sum_{j&lt;i} \\sigma_j \\delta_j\\right)\\). \\(\\widehat{\\mathbf{C}}\\) conditioned on \\(\\sigma\\), \\(\\mathbf{c}\\), and \\(T\\) is conditioned on \\(\\sigma\\) to simplify follow-up descriptions. We denote the contribution of a point to the cumulative color as its weight \\(\\omega_i\\):</p> \\[ \\omega_i=T_i(\\sigma)\\left(1-\\exp \\left(-\\sigma_i \\delta_i\\right)\\right) \\] <p>NeRF is optimized by minimizing the photometric loss:</p> \\[ \\mathcal{L}_{p m}=\\|\\widehat{\\mathbf{C}}-\\mathbf{C}\\|_2 \\]"},{"location":"neural-graphics/#physical-meanings-of-the-terms-in-the-volume-rendering-equation","title":"Physical meanings of the terms in the volume rendering equation","text":"<p>Alpha value of a point: \\(\\alpha_i = 1-\\exp \\left(-\\sigma_i \\delta_i\\right)\\)</p> <p>Contribution of a point to the cumulative color: \\(\\omega_i = T_i \\alpha_i\\)</p> <p>Opacity of each ray: \\(\\tau = \\sum_i \\omega_i\\)</p> <p>Depth of each ray: \\(d = \\sum_i \\omega_i t_i\\), where \\(t_i\\) is the distance between the i-th point and the camera</p>"},{"location":"neural-graphics/#flow-matching","title":"Flow Matching","text":"<p>Let \\(\\mathbb{R}^d\\) denotes the data space with data points \\(x = (x^1, \\cdots ,x^d) \\in \\mathbb{R}^d\\).</p> <p>Think of \\(x\\) as a location or sample point in the space where your data lives: If your data are 2D points, then \\(d=2\\) and each \\(x\\) is a point in the 2D plane; if your data are images of size 28x28, then \\(d=784\\) and each \\(x\\) is a vector representing pixel values.</p> <p>Two important objects we use in this paper are: the probability density path \\(p: [0,1] \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}_{&gt;0}\\) which is a time dependent probability density function, i.e., \\(\\int p_t(x)dx = 1\\), and a time-dependent vector field \\(v: [0,1] \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^d\\).</p> <p>In mathematics, writing \\(f: A \\rightarrow B\\) means that \\(f\\) is a function that takes inputs from set \\(A\\) and produces outputs in set \\(B\\). So here, \\(p\\) is a function that takes two inputs: a time variable from the interval \\(t \\in [0,1]\\) and a position (data point) \\(x \\in \\mathbb{R}^d\\), and it outputs a positive real number \\(\\mathbb{R}_{&gt;0}\\) (the probability density at that point and time). \\(p(x, t)\\) or equivalently \\(p_t(x)\\) denotes the probability density of \\(x\\) at \\(t\\).</p> <p>A vector field \\(v_t\\) can be used to construct a time-dependent diffeomorphic map, called a flow, \\(\\phi: [0,1] \\times R_d \\rightarrow R_d\\), defined via the ordinary differential equation (ODE):</p>"},{"location":"tensor/","title":"Tensor Operations","text":""},{"location":"tensor/#tensor-creation","title":"Tensor Creation","text":""},{"location":"tensor/#dimension-operations","title":"Dimension Operations","text":""},{"location":"tensor/#add-a-new-dimension","title":"Add a new dimension","text":"<p>unsqueeze()</p> <pre><code>t = torch.rand((3,4))  ## shaped [3, 4]\n\nt_new = t.unsqueeze(0) ## shaped [1, 3, 4]\nt_new = t.unsqueeze(1) ## shaped [3, 1, 4]\nt_new = t.unsqueeze(2) ## shaped [3, 4, 1]\n</code></pre> <p>view()</p> <pre><code>t = torch.rand(2)  ## shaped [2]\n\nt_new = t.view(1, 1, 2) ## shaped [1, 1, 2]\n</code></pre>"},{"location":"tensor/#activate-function","title":"Activate Function","text":""},{"location":"tensor/#sigmoid","title":"Sigmoid","text":"<p>torch.sigmoid</p> <p>Computes the logistic sigmoid function of the elements of input.</p> \\[ \\text{out}_i = \\frac{1}{1 + e^{-\\text{input}_i}} \\] <pre><code>t = torch.randn(4)\ntorch.sigmoid(t)\n</code></pre>"},{"location":"tensor/#log_softmax","title":"log_softmax","text":"\\[ \\text{log\\_softmax}(x_i) = \\log \\left( \\frac{e^{x_i}}{\\sum_{j} e^{x_j}} \\right) = x_i - \\log\\left(\\sum_{j} e^{x_j}\\right) \\] <pre><code>import torch\n</code></pre> <pre><code>def log_softmax(input: torch.Tensor, dim: int = -1) -&gt; torch.Tensor:\n    \"\"\"\n    Compute log_softmax\n\n    Args:\n        input: Input tensor\n        dim: Dimension along which to compute log_softmax\n             (only -1 or last dim supported)\n    Returns:\n        Tensor with log_softmax applied along the specified dimension\n    \"\"\"\n    if dim != -1 and dim != input.ndim - 1:\n        raise ValueError(\n            \"This implementation only supports log_softmax along the last dimension\"\n        )\n\n    # Flatten all dimensions except the last one\n    original_shape = input.shape\n    input_2d = input.reshape(-1, input.shape[-1])\n    input_2d = input_2d.contiguous()\n\n    n_rows, n_cols = input_2d.shape\n\n    # calculate max per row for numerical stability\n    max_per_row = torch.max(input_2d, dim=-1, keepdim=True).values\n    input_stable = input_2d - max_per_row\n    exp_input = torch.exp(input_stable)\n    sum_exp = torch.sum(exp_input, dim=-1, keepdim=True)\n    log_sum_exp = torch.log(sum_exp)\n    log_softmax_2d = input_stable - log_sum_exp - max_per_row\n    log_softmax_output = log_softmax_2d.reshape(original_shape)\n    return log_softmax_output\n</code></pre> <p>Compute the maximum value in the input tensor and subtract it from all elements for numerical stability.</p> \\[ x'_i = x_i - \\max_j(x_j) \\] <p>calculate the sum of exponentials of the values. $$ \\text{sum_exp} = \\sum_j e^{x'_j} = \\sum_j e^{x_j} \\cdot e^{-\\max_j(x_j)} $$</p> \\[ \\text{log\\_sum\\_exp} = \\log(\\text{sum\\_exp}) = \\log\\left(\\sum_j e^{x_j}\\right) - \\max_j(x_j) \\] <p>Finally, compute the log-softmax values using the stabilized inputs.</p> \\[ \\text{log\\_softmax}(x_i) = x'_i - \\text{log\\_sum\\_exp} = x_i - \\log\\left(\\sum_j e^{x_j}\\right) \\]"},{"location":"llms/models/","title":"Models","text":""},{"location":"llms/models/#qwen3-omni","title":"Qwen3-Omni","text":""},{"location":"llms/models/#overview","title":"Overview","text":"<p>Main features of Qwen3-Omni:</p> <ul> <li>Both the Thinker and Talker adopt Mixture-of-Experts (MoE) architectures.</li> <li>Talker no longer consumes the Thinker\u2019s high-level text representations and conditions only on audio and visual multimodal features.</li> <li>for textual content, discrete tokens and embeddings are effectively information-equivalent.</li> <li>Since textual representations are decoupled, the Thinker and Talker can use distinct system prompts, independently controlling the Thinker\u2019s response style and the Talker\u2019s audio style.</li> <li>The Talker adopts a multi-codebook autoregressive scheme: Talker generates one codec frame per step, while the MTP module produces the remaining residual codebooks.</li> <li>The Code2Wav is implemented as a lightweight causal ConvNet, simplifying the final stage of audio synthesis.</li> </ul>"},{"location":"llms/models/#audio-transformer-aut","title":"Audio Transformer (AuT)","text":"<ul> <li>Audio Transformer (AuT) is an attention-encoder-decoder model, trained from scratch on 20 million hours of supervised audio data.</li> <li>AuT Encoder contains approximately 0.6B parameters.</li> <li>Filter bank features are time\u2013frequency representations derived from short-time analysis of the waveform. A 10 ms frame shift (hop) means the analysis window advances by 10 ms each step.</li> <li>In Qwen3-Omni\u2019s description, for audio inputs and audio extracted from video, we resample to 16 kHz and convert the raw waveform into a 128 channel mel-spectrogram with a 25 ms window and a 10 ms hop. Practically, this corresponds to overlapping frames such as 0\u201325 ms, 10\u201335 ms, 20\u201345 ms, etc. A common way to compute a mel-spectrogram is:<ul> <li>segment the waveform into overlapping frames (e.g., 25 ms window, 10 ms hop),</li> <li>apply an FFT (or equivalent STFT) per frame to obtain a magnitude/power spectrum,</li> <li>apply a mel-scaled filter bank to aggregate spectral energy into mel bands,</li> <li>optionally apply log-compression (often used in practice, though not always explicitly stated).</li> </ul> </li> <li>The filter bank features of the audio are downsampled 8 times using Conv2D blocks before the attention layers, from 100 Hz(10ms) to 12.5 Hz. Each frame of the audio representation corresponds to approximately an 80 ms segment of the original audio signal.</li> <li>Specifically, the training data includes 80% Chinese and English pseudo-labeled ASR data, 10% ASR data from other languages, and 10% audio understanding data.</li> <li>To balance the efficiency of real-time prefill caching with the performance for offline audio tasks, AuT utilizes flash attention with dynamic attention window sizes, covering attention query patterns ranging from 1 to 8 seconds.</li> </ul>"},{"location":"llms/preliminary/","title":"AI Infra Preliminary","text":""},{"location":"llms/preliminary/#metrics","title":"Metrics","text":""},{"location":"llms/preliminary/#latency","title":"Latency","text":"<p>Latency is the time taken to process a request and generate output tokens. In LLM inference, latency is often measured in two main metrics:</p>"},{"location":"llms/preliminary/#time-to-first-token-ttft","title":"Time to first token (TTFT)","text":"<p>Time to First Token (TTFT) is the latency between when a request is received by the inference server and when the first output token is generated and returned to the client. It measures how quickly the model begins responding, and is a primary user-perceived latency metric in streaming LLM systems. Formally:</p> \\[ \\text{TTFT} = t_{\\text{first token emitted}} - t_{\\text{request received}} \\] <p>TTFT mainly captures the prefill phase (processing the input context) plus scheduling and system overhead. It does not include the time required to generate the rest of the output tokens. In details TTFT can be decomposed into:</p> \\[ \\small \\begin{aligned} \\text{TTFT} = t_{\\text{queue}} + t_{\\text{input processing}} + t_{\\text{prefill compute}} + t_{\\text{first decode step (optional)}} + t_{\\text{framework overhead}} \\end{aligned} \\] <p>where:</p> <ul> <li>\\(t_{\\text{queue}}\\) is the time spent waiting in the scheduler before the request is admitted for execution.</li> <li>\\(t_{\\text{input processing}}\\) includes tokenization, multimodal preprocessing (e.g., image or video encoding), and any request-level normalization or formatting.</li> <li>\\(t_{\\text{prefill compute}}\\) is the time required to run the forward pass over all input tokens (complexity is \\(O(L)\\) in sequence length \\(L\\)), during which the KV cache is populated and the logits at the final prompt position are produced.</li> <li>\\(t_{\\text{first decode step (optional)}}\\) accounts for an additional single-step decode iteration in implementations that separate prefill and generation scheduling. In such systems, the first output token is emitted during a subsequent decode pass rather than directly from the prefill logits. In optimized implementations (recent versions of vLLM), this step may be fused with prefill and therefore negligible.</li> <li>\\(t_{\\text{framework overhead}}\\) captures auxiliary costs such as CUDA kernel launches, synchronization, memory allocation, sampling (softmax and top-k/top-p), and network transmission of the first streamed token.</li> </ul> <p>For long-context inputs, TTFT is typically dominated by \\(t_{\\text{prefill compute}}\\), with the remaining terms contributing comparatively small overhead.</p>"},{"location":"llms/preliminary/#time-per-output-token-tpot","title":"Time per output token (TPOT)","text":"<p>Time per Output Token (TPOT) measures the average latency required to generate each token after the first token has been emitted. It characterizes the steady-state decode performance of an autoregressive model and directly determines streaming speed. Formally, for a request generating \\(N\\) output tokens:</p> \\[ \\text{TPOT} = \\frac{t_\\text{last token emitted} - t_\\text{first token emitted}}{N - 1} \\] <p>Equivalently, if total end-to-end latency is \\(T_{\\text{total}}\\),</p> \\[ \\text{TPOT} = \\frac{T_{\\text{total}} - \\text{TTFT}}{N - 1} \\] <p>In steady-state decoding, each output token requires:</p> \\[ \\text{TPOT} = t_{\\text{decode compute}} + t_{\\text{communication}} + t_{\\text{scheduling}} + t_{\\text{framework overhead}} \\] <p>where:</p> <ul> <li>\\(t_{\\text{decode compute}}\\) is the forward pass for a single token using the KV cache (complexity per step is \\(O(1+\\text{num speculative tokens})\\) in sequence length but proportional to model size).</li> <li>\\(t_{\\text{communication}}\\) captures the cost of cross-GPU synchronization and data exchange, such as tensor-parallel collectives (all-reduce, all-gather), expert routing and aggregation in MoE models (all-to-all, dispatch/combine), as well as any host\u2013device memory transfers (H2D/D2H) triggered by scheduling or memory management.</li> <li>\\(t_{\\text{scheduling}}\\) includes runtime scheduling overhead such as continuous batching, request coordination, admission control, and KV cache allocation, paging, or memory management.</li> <li>\\(t_{\\text{framework overhead}}\\) includes auxiliary execution costs such as sampling (softmax, top-k/top-p), kernel launches, synchronization, input/output marshaling, post-processing of results, and token streaming.</li> </ul> <p>Unlike TTFT, TPOT does not require recomputation over the full prompt. However, it still depends on the effective context length, since each decode step performs attention over all cached tokens. Therefore, TPOT scales approximately linearly with the total context length (prompt + generated tokens), though the per-token cost is significantly smaller than prefill.</p>"},{"location":"llms/preliminary/#throughput","title":"Throughput","text":""},{"location":"llms/preliminary/#tokens-per-second-tps","title":"Tokens per Second (TPS)","text":"<p>Tokens per Second (TPS) measures the throughput of an LLM inference system, i.e., how many tokens are processed or generated per second under a given serving configuration. Unlike TTFT and TPOT, which are latency metrics defined at the request level, TPS is a system-level metric that reflects overall serving capacity under load. Over a time interval over a time interval \\(\\Delta t\\), TPS can be expressed as:</p> \\[ \\text{TPS} = \\frac{N_\\text{tokens}}{\\Delta t} \\] <p>In many cases, TPS refers specifically to decode throughput, counting only generated output tokens:</p> \\[ \\text{TPS}_\\text{decode} = \\frac{N_\\text{output tokens}}{\\Delta t} \\] <p>and for a single sequence decoded sequentially, it is approximately the inverse of TPOT:</p> \\[ \\text{TPS}_\\text{per request} \\approx \\frac{1}{\\text{TPOT}} \\] <p>However, in practical serving systems with continuous batching and concurrent decoding, aggregate throughput scales with the effective batch size \\(B\\). For a single replica, the system throughput can be approximated as</p> \\[ \\text{TPS}_\\text{replica} \\approx \\frac{B}{\\text{TPOT}} \\] <p>and with \\(R\\) data-parallel (DP) replicas,</p> \\[ \\text{TPS}_\\text{total} \\approx R \\times \\frac{B}{\\text{TPOT}} \\] <p>Importantly, for long-context or multimodal workloads, prompt processing can contribute a substantial fraction of total computation. In such cases, it is often necessary to consider end-to-end throughput, which accounts for both prompt and generated tokens:</p> \\[ \\text{TPS}_\\text{end-to-end} = \\frac{N_\\text{prompt tokens} + N_\\text{output tokens}}{\\Delta t} \\] <p>Overall, TPS is influenced by model size, parallelism strategy, effective batch size, kernel efficiency, interconnect bandwidth, and the relative balance between prefill and decode workloads.</p>"},{"location":"llms/preliminary/#kv-cache","title":"KV Cache","text":""},{"location":"llms/preliminary/#memory-cost","title":"Memory cost","text":"<p>The KV cache size is calculated as:</p> \\[ \\text{KV size} = 2 \\times \\text{num layers} \\times \\text{num heads} \\times \\text{seq len} \\times \\text{head dim} \\] <p>Assume:</p> <ul> <li>32 layers, 80 heads, head_dim = 128, seq_len = 4K.</li> <li>FP16 KV cache: 2 \u00d7 32 \u00d7 80 \u00d7 4K \u00d7 128 \u00d7 2 bytes = ~5.2 GB per sample.</li> <li>C8 (int8 KV cache): half that \u2192 ~2.6 GB per sample.</li> </ul>"},{"location":"llms/preliminary/#attention","title":"Attention","text":"<p>In transformer models, the attention module computes:</p> \\[ \\operatorname{Attention}(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V \\] <p>Every token produces:</p> <ul> <li>Q (Query) \u2014 derived from the current token\u2019s hidden state</li> <li>K (Key), V (Value) \u2014 derived from the context (previous tokens) and stored in the KV cache</li> </ul> <p>The difference between MHA, MQA, and GQA lies in how many Key/Value projections are used relative to the number of Query heads.</p>"},{"location":"llms/preliminary/#mha-multi-head-attention","title":"MHA \u2014 Multi-Head Attention","text":"<p>Each attention head has its own set of projections:</p> \\[ Q_i=X W_{Q, i}, \\quad K_i=X W_{K, i}, \\quad V_i=X W_{V, i} \\] <ul> <li>Q heads = K heads = V heads</li> </ul>"},{"location":"llms/preliminary/#mqa-multi-query-attention","title":"MQA \u2014 Multi-Query Attention","text":"<p>All heads share one K and V projection:</p> \\[ K_{\\text {shared }}=X W_K, \\quad V_{\\text {shared }}=X W_V \\] <ul> <li>Multiple Q heads, but only one pair of shared K/V.</li> <li>During inference, the same K/V are reused for all heads.</li> <li>KV cache size reduced by ~#heads\u00d7</li> <li>LLaMA 2 7B, Mistral, and most optimized LLMs use MQA to cut memory usage.</li> </ul>"},{"location":"llms/preliminary/#gqa-grouped-query-attention","title":"GQA \u2014 Grouped-Query Attention","text":"<p>Queries are split into groups, and each group shares its own K/V:</p> \\[ \\text { Group } g: \\quad Q_{g, *}, K_g, V_g \\] <ul> <li>Suppose you have 8 query heads but only 2 groups \u2192 4 Q heads per group share one K/V.</li> <li>K/V heads &lt; Q heads.</li> <li>LLaMA 2 70B, Mixtral, GPT-J-6B, etc. use GQA for better memory efficiency.</li> </ul>"},{"location":"llms/preliminary/#sampling","title":"Sampling","text":""},{"location":"llms/preliminary/#temperature","title":"Temperature","text":"<p>Temperature \\(T\\) controls \"sharpness\" of the probability distribution. Given model logits \\(z_i\\), temperature sampling rescales them before softmax:</p> \\[ p_i=\\frac{e^{z_i / T}}{\\sum_j e^{z_j / T}} \\] <ul> <li>\\(T&lt;1\\) Sharper (more peaked). Model becomes confident, less random. \\(T&gt;1\\) Flatter (more uniform). Model becomes exploratory, more diverse.</li> <li>Temperature keeps the order of logits because dividing by a positive number does not change ranking (rank-preserving). If you use top-k = 1 (i.e., greedy), temperature has no effect at all.</li> <li>With a temperature \\(T \\rightarrow 0\\), the softmax distribution becomes arbitrarily sharp and converges to greedy (argmax) sampling. For example, at \\(T=10^{-6}\\), the highest logit dominates the distribution, so sampling is effectively greedy in practice.</li> <li>At \\(T \\rightarrow \\infty\\), the distribution becomes uniform (all tokens equally likely).</li> </ul>"},{"location":"llms/preliminary/#top-k-sampling","title":"Top-k Sampling","text":"<p>After computing probabilities (after temperature), sort tokens by probability, and keep only the k highest. Then renormalize \\(p_i^{\\prime}\\) to get final distribution for sampling.</p> \\[ \\begin{aligned} S_k &amp; =\\text { top-k tokens by } p_i \\\\ p_i^{\\prime} &amp; = \\begin{cases}\\frac{p_i}{\\sum_{j \\in S_k} p_j} &amp; i \\in S_k \\\\ 0 &amp; \\text { otherwise }\\end{cases} \\end{aligned} \\] <p>Then you sample from that truncated distribution.</p> <ul> <li>Small k (e.g., 1\u201310): deterministic and safe (model never picks rare words).</li> <li>Large k (e.g., 50\u2013100): more variety but higher risk of off-topic words.</li> </ul>"},{"location":"llms/preliminary/#top-p-nucleus-sampling","title":"Top-p (nucleus) Sampling","text":"<p>\u201cnucleus\u201d literally means core, center, or the essential central part of something. Instead of a fixed k, we choose the smallest set of tokens whose cumulative probability \\(\\geq p\\).</p> \\[ \\begin{aligned} S_p&amp;=\\left\\{i: \\sum_{j \\in S_p} p_j \\geq p\\right\\} \\\\ p_i^{\\prime}&amp;= \\begin{cases}\\frac{p_i}{\\sum_{j \\in S_p} p_j} &amp; i \\in S_p \\\\ 0 &amp; \\text { otherwise }\\end{cases} \\end{aligned} \\] <p>Key difference vs top-k:</p> <p>Top-p adapts to uncertainty. Top-k is about rank (k highest scores). Top-p is about mass (the core mass of the distribution).</p> <ul> <li>If the model is confident (one token dominates), very few tokens are kept.</li> <li>If it\u2019s uncertain (many tokens have similar probabilities), the nucleus expands.</li> </ul>"},{"location":"llms/preliminary/#min-p","title":"Min-p","text":"<p>Min-p filters out tokens that are too improbable relative to the top-1. This is newer but increasingly popular (used in Mistral, Gemini, etc.). Instead of fixing \\(k\\) or \\(p\\), you keep tokens above a minimum probability threshold relative to the top-1 token:</p> \\[ S_{\\min }=\\left\\{i: p_i \\geq \\min -\\mathrm{p} \\times p_{\\max }\\right\\} \\] <p>If min-p = 0.1, then any token whose probability is at least 10% of the most likely token is kept.Then normalize:</p> \\[ p_i^{\\prime} = \\frac{p_i}{\\sum_{j \\in S_{\\min}} p_j} \\] <ul> <li>Min-p adaptive to context like top-p, but simpler. It means the number of candidate tokens changes depending on what the model is predicting. If the model is confident, the candidate set is small. If the model is uncertain, the candidate set becomes larger.</li> <li>Avoids cutting off plausible low-rank tokens when the distribution is flat.</li> <li>More numerically stable for long-tail vocabularies.</li> </ul>"},{"location":"llms/preliminary/#put-them-together","title":"Put them together","text":"<p>You usually combine these in a specific order: Temperature \u2192 Softmax \u2192 (top-k / top-p / min-p) \u2192 Renormalize \u2192 Sample</p> <p>When both top-k and top-p are enabled, you always keep the intersection, which is the smaller of the two sets.</p>"},{"location":"llms/preliminary/#example-code","title":"Example code","text":"<p>Code snippet from Omniinfer Sampler.</p> Top-k and Top-p implementation <pre><code>def apply_top_k_top_p(\n    logits_or_prob: torch.Tensor,\n    k: Optional[torch.Tensor],\n    p: Optional[torch.Tensor],\n    is_logits: bool,\n) -&gt; torch.Tensor:\n    if p is None:\n        if k is not None:\n            logits_or_prob = apply_top_k_only(logits_or_prob, k, is_logits)\n        if is_logits:\n            probs = logits_or_prob.softmax(dim=-1, dtype=torch.float32)\n        else:\n            probs = logits_or_prob / logits_or_prob.sum(dim=-1, keepdim=True)\n        return probs, None\n\n    logits_or_prob_sort, logits_or_prob_idx = logits_or_prob.sort(dim=-1, descending=False)\n\n    if k is not None:\n        # Apply top-k.\n        top_k_mask = logits_or_prob_sort.size(1) - k.to(torch.long)  # shape: B\n        # Get all the top_k values.\n        top_k_mask = logits_or_prob_sort.gather(1, top_k_mask.unsqueeze(dim=1))\n        top_k_mask = logits_or_prob_sort &lt; top_k_mask\n        logits_or_prob_sort.masked_fill_(top_k_mask, -float(\"inf\") if is_logits else 0)\n\n    # Apply top-p.\n    if is_logits:\n        probs_sort = logits_or_prob_sort.softmax(dim=-1)\n    else:\n        probs_sort = logits_or_prob_sort / logits_or_prob_sort.sum(dim=-1, keepdim=True)\n    probs_sum = torch.cumsum(probs_sort, dim=-1, out=probs_sort)\n    top_p_mask = probs_sum &lt;= 1 - p.unsqueeze(dim=1)\n    # at least one\n    top_p_mask[:, -1] = False\n    probs_sort.masked_fill_(top_p_mask, 0)\n    probs = probs_sort / probs_sort.sum(dim=-1, keepdim=True)\n    return probs, logits_or_prob_idx\n</code></pre> <pre><code>def apply_top_k_only(\n    logits_or_prob: torch.Tensor,\n    k: torch.Tensor,\n    is_logits: bool,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Apply top-k mask to the logits.\n\n    This implementation doesn't involve sorting the entire vocab.\n\n    The logits tensor may be updated in-place.\n    \"\"\"\n    no_top_k_mask = k == logits_or_prob.shape[1]\n    # Set non-top-k rows to 1 so that we can gather.\n    k = k.masked_fill(no_top_k_mask, 1)\n    max_top_k = k.max()\n    # topk.values tensor has shape [batch_size, max_top_k].\n    # Convert top k to 0-based index in range [0, max_top_k).\n    k_index = k.sub_(1).unsqueeze(1)\n    top_k_mask = logits_or_prob.topk(max_top_k, dim=1).values.gather(1, k_index.long())\n    # Handle non-topk rows.\n    top_k_mask.masked_fill_(no_top_k_mask.unsqueeze(1), -float(\"inf\"))\n    logits_or_prob.masked_fill_(logits_or_prob &lt; top_k_mask, -float(\"inf\") if is_logits else float(0))\n    return logits_or_prob\n</code></pre>"},{"location":"llms/preliminary/#speculvative-decoding","title":"Speculvative decoding","text":""},{"location":"llms/preliminary/#rejection-sampler","title":"Rejection Sampler","text":"<p>To sample \\(x \\sim p(x)\\),  we instead sample \\(x \\sim q(x)\\), keeping it if \\(q(x) \\leq p(x)\\), and in case \\(q(x) &gt; p(x)\\), we reject the sample with probability \\(1 - \\frac{p(x)}{q(x)}\\) and resample \\(x\\)  from an adjusted distribution \\(p^{\\prime} = \\text{norm}(\\max(0, p(x) - q(x)))\\) instead.</p> <p>NOTE: For for any distributions \\(p(x)\\) and \\(q(x)\\), the tokens sampled via rejection sampling from \\(p(x)\\) and \\(q(x)\\) are distributed identically to those sampled from \\(p(x)\\) alone. The proof is in Appendix A [1].</p> <p>Rejection sampling works by decomposing the target distribution \\(p(x)\\) into two parts:</p> <ul> <li>The accept branch, where tokens are accepted based on the ratio \\(\\frac{p(x)}{q(x)}\\), where \\(q(x)\\) is the proposal (draft) distribution.</li> </ul> \\[   p(\\text{accepted}, x = x^{\\prime}) = q(x^{\\prime}) \\cdot \\min\\left(\\frac{p(x^{\\prime})}{q(x^{\\prime})}, 1\\right) = \\min(p(x^{\\prime}), q(x^{\\prime})) \\] <ul> <li>The reject branch, where tokens are rejected and resampled from the adjusted distribution \\(p^{\\prime}(x)\\). The probability of all tokens being sampled from \\(q(x)\\) being rejected as:</li> </ul> \\[ \\begin{aligned}     p(\\text{rejected}) &amp;= \\sum_{x} q(x) \\left(1 - \\min\\left(\\frac{p(x)}{q(x)}, 1\\right)\\right) \\\\     &amp;= \\sum_{x} \\max(0, q(x) - p(x)) \\\\ \\end{aligned} \\] \\[ \\begin{aligned}     p(\\text{rejected}) &amp;= \\sum_{x} q(x) \\left(1 - \\min\\left(\\frac{p(x)}{q(x)}, 1\\right)\\right) \\\\     &amp;= \\sum_{x} \\max(0, q(x) - p(x)) \\\\ \\end{aligned} \\] <p>Example Code</p> <p>Code snippet from vLLM.</p> Rejection sampling kernel in Triton <pre><code># NOTE(woosuk): Avoid specialization to prevent unnecessary recompilation.\n@triton.jit(do_not_specialize=[\"max_spec_len\"])\ndef rejection_random_sample_kernel(\n    output_token_ids_ptr,  # [batch_size, max_spec_len + 1]\n    cu_num_draft_tokens_ptr,  # [batch_size]\n    draft_token_ids_ptr,  # [num_tokens]\n    draft_probs_ptr,  # [num_tokens, vocab_size] or None\n    target_probs_ptr,  # [num_tokens, vocab_size]\n    bonus_token_ids_ptr,  # [batch_size]\n    recovered_token_ids_ptr,  # [num_tokens]\n    uniform_probs_ptr,  # [num_tokens]\n    is_greedy_ptr,  # [batch_size]\n    max_spec_len,\n    vocab_size,\n    NO_DRAFT_PROBS: tl.constexpr,\n):\n    req_idx = tl.program_id(0)\n    is_greedy = tl.load(is_greedy_ptr + req_idx)\n    if is_greedy:\n        # Early exit for greedy sampling requests.\n        return\n\n    start_idx = 0 if req_idx == 0 else tl.load(cu_num_draft_tokens_ptr + req_idx - 1)\n    end_idx = tl.load(cu_num_draft_tokens_ptr + req_idx)\n    num_draft_tokens = end_idx - start_idx\n\n    rejected = False\n    for pos in range(num_draft_tokens):\n        if not rejected:\n            draft_token_id = tl.load(draft_token_ids_ptr + start_idx + pos)\n            if NO_DRAFT_PROBS:\n                draft_prob = 1\n            else:\n                draft_prob = tl.load(\n                    draft_probs_ptr + (start_idx + pos) * vocab_size + draft_token_id\n                )\n            target_prob = tl.load(\n                target_probs_ptr + (start_idx + pos) * vocab_size + draft_token_id\n            )\n            uniform_prob = tl.load(uniform_probs_ptr + start_idx + pos)\n            # NOTE(woosuk): While the draft probability should never be 0,\n            # we check it to avoid NaNs. If it happens to be 0, we reject.\n            if draft_prob &gt; 0 and target_prob / draft_prob &gt;= uniform_prob:\n                # Accept.\n                token_id = draft_token_id\n            else:\n                # Reject. Use recovered token.\n                rejected = True\n                token_id = tl.load(recovered_token_ids_ptr + start_idx + pos)\n            tl.store(\n                output_token_ids_ptr + req_idx * (max_spec_len + 1) + pos, token_id\n            )\n\n    if not rejected:\n        # If all tokens are accepted, append the bonus token.\n        bonus_token_id = tl.load(bonus_token_ids_ptr + req_idx)\n        tl.store(\n            output_token_ids_ptr + req_idx * (max_spec_len + 1) + num_draft_tokens,\n            bonus_token_id,\n        )\n</code></pre> Resample kernel in Triton <pre><code>@triton.jit\ndef sample_recovered_tokens_kernel(\n    output_token_ids_ptr,  # [num_tokens]\n    cu_num_draft_tokens_ptr,  # [batch_size]\n    draft_token_ids_ptr,  # [num_tokens]\n    draft_probs_ptr,  # [num_tokens, vocab_size] or None\n    target_probs_ptr,  # [num_tokens, vocab_size]\n    q_ptr,  # [batch_size, vocab_size]\n    vocab_size,\n    PADDED_VOCAB_SIZE: tl.constexpr,\n    NO_DRAFT_PROBS: tl.constexpr,\n):\n    req_idx = tl.program_id(0)\n    start_idx = 0 if req_idx == 0 else tl.load(cu_num_draft_tokens_ptr + req_idx - 1)\n    end_idx = tl.load(cu_num_draft_tokens_ptr + req_idx)\n    num_draft_tokens = end_idx - start_idx\n\n    # Early exit for out-of-range positions.\n    pos = tl.program_id(1)\n    if pos &gt;= num_draft_tokens:\n        return\n\n    vocab_offset = tl.arange(0, PADDED_VOCAB_SIZE)\n    if NO_DRAFT_PROBS:\n        draft_token_id = tl.load(draft_token_ids_ptr + start_idx + pos)\n        prob = tl.load(\n            target_probs_ptr + (start_idx + pos) * vocab_size + vocab_offset,\n            mask=((vocab_offset &lt; vocab_size) &amp; (vocab_offset != draft_token_id)),\n            other=0,\n        )\n    else:\n        draft_prob = tl.load(\n            draft_probs_ptr + (start_idx + pos) * vocab_size + vocab_offset,\n            mask=vocab_offset &lt; vocab_size,\n            other=0,\n        )\n        target_prob = tl.load(\n            target_probs_ptr + (start_idx + pos) * vocab_size + vocab_offset,\n            mask=vocab_offset &lt; vocab_size,\n            other=0,\n        )\n        prob = tl.maximum(target_prob - draft_prob, 0)\n        # NOTE(woosuk): We don't need `prob = prob / tl.sum(prob)` here because\n        # `tl.argmax` will select the maximum value.\n\n    q = tl.load(\n        q_ptr + req_idx * vocab_size + vocab_offset,\n        mask=vocab_offset &lt; vocab_size,\n        other=float(\"-inf\"),\n    )\n    recovered_id = tl.argmax(prob / q, axis=-1)\n    tl.store(output_token_ids_ptr + start_idx + pos, recovered_id)\n</code></pre> <p>Reference</p> <p>[1] Y Leviathan, M Kalman, Y Matias, \"Fast Inference from Transformers via Speculative Decoding\", ICML 2023.</p>"},{"location":"llms/preliminary/#pre-training-objective-of-llms","title":"Pre-training Objective of LLMs","text":"<p>A large language model trained from scratch is optimized using maximum likelihood estimation (MLE) over a large text corpus.</p> <p>Given a sequence of tokens \\((t_1, t_2, \\ldots, t_n)\\) sampled from the data distribution \\(p_\\text{data}\\) the model defines an autoregressive factorization:</p> \\[ p_\\theta(t_1, t_2, \\ldots, t_n) = \\prod_{i=1}^n p_\\theta(t_i | t_1, t_2, \\ldots, t_{i-1}) \\] <p>The training objective is to maximize the log-likelihood of the data, which is equivalent to minimizing the negative log-likelihood (NLL):</p> \\[ \\mathcal{L}(\\theta) = - \\mathbb{E}_{(t_1, t_2, \\ldots, t_i) \\sim p_\\text{data}} \\left[ \\log p_\\theta(t_i | t_1, t_2, \\ldots, t_{i-1}) \\right] \\] <p>In practice, this loss is implemented as token-level cross-entropy with one-hot targets:</p> \\[ \\mathcal{L} = -\\frac{1}{N} \\sum_{\\text{tokens}} \\log p_\\theta(t_i | t_1, t_2, \\ldots, t_{i-1}) \\] <p>This objective corresponds to minimizing the cross-entropy between the empirical data distribution and the model distribution. or equivalently, minimizing \\(KL(p_\\text{data} || p_\\theta)\\).</p>"}]}